{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BloPBjjSqqUO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8vejTOLLqtw_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/movies_dataset_2000_2023.csv')\n",
    "#df = pd.read_csv('movies_dataset_2000_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6aKhjRNrERf",
    "outputId": "098d334b-a8bb-49bf-b20d-d5162dbdde40",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6095, 24)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pWsXfE_4rEp7",
    "outputId": "abc4ce12-fd3c-4670-ae24-818f8a6f40df",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Movie Title', 'Plot Summary', 'Cover Image', 'Year',\n",
       "       'Source', 'Rated', 'omdb Year', 'omdb Title', 'Released', 'Runtime',\n",
       "       'Genre', 'Director', 'Writer', 'Actors', 'Plot', 'Language', 'Country',\n",
       "       'Awards', 'Poster', 'Ratings', 'imdbRating', 'BoxOffice', 'imdbID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5C7tcH2vrHcE",
    "outputId": "3773e77e-2269-4eb9-f30c-193c517d362a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6095 entries, 0 to 6094\n",
      "Data columns (total 24 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0.1  896 non-null    float64\n",
      " 1   Movie Title   6095 non-null   object \n",
      " 2   Plot Summary  5556 non-null   object \n",
      " 3   Cover Image   5990 non-null   object \n",
      " 4   Year          6095 non-null   int64  \n",
      " 5   Source        6095 non-null   object \n",
      " 6   Rated         841 non-null    object \n",
      " 7   omdb Year     871 non-null    object \n",
      " 8   omdb Title    871 non-null    object \n",
      " 9   Released      868 non-null    object \n",
      " 10  Runtime       861 non-null    object \n",
      " 11  Genre         870 non-null    object \n",
      " 12  Director      850 non-null    object \n",
      " 13  Writer        857 non-null    object \n",
      " 14  Actors        867 non-null    object \n",
      " 15  Plot          862 non-null    object \n",
      " 16  Language      866 non-null    object \n",
      " 17  Country       869 non-null    object \n",
      " 18  Awards        756 non-null    object \n",
      " 19  Poster        864 non-null    object \n",
      " 20  Ratings       871 non-null    object \n",
      " 21  imdbRating    860 non-null    float64\n",
      " 22  BoxOffice     762 non-null    object \n",
      " 23  imdbID        871 non-null    object \n",
      "dtypes: float64(2), int64(1), object(21)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJ497_2FsFC3",
    "outputId": "e36c5bfd-6d15-49df-d7bd-1afb0e691ea5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JS3MptPLsJuo",
    "outputId": "07654457-c0bf-4fed-9cf0-37a1a9fcfb56",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shreyabaral/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shreyabaral/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNuBr4Y2rI-4",
    "outputId": "33bfa17d-738f-4ffb-cab9-2c66394327a8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shreyabaral/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import string library for text preprocessing\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7pTcfGq1EoDv"
   },
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "def convert_to_lower_case(text):\n",
    "  return text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dJ6qknuHEvec"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "  # Create a translation table to remove punctuation\n",
    "  translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "  # Remove punctuation using the translation table\n",
    "  text_without_punct = text.translate(translator)\n",
    "\n",
    "  return text_without_punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3oAnu0IuE0go"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "  # Tokenization\n",
    "  return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "L6JfRRBaE2AK"
   },
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "def remove_stop_words(tokens):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  return [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnMNeXNuIeGp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BQoEugPcE7-f"
   },
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "  # Lemmatization\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  return [lemmatizer.lemmatize(word) if lemmatizer.lemmatize(word) is not None else word for word in tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DjFO6o1eE--o"
   },
   "outputs": [],
   "source": [
    "# Remove special characters and numbers\n",
    "def remove_special_chars(tokens):\n",
    "  return [re.sub('[^A-Za-z]+', '', word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eEMuVaI_FJDK"
   },
   "outputs": [],
   "source": [
    "# function to do preprocessing text\n",
    "def preprocess_text(text):\n",
    "  lower_text = convert_to_lower_case(text) # converting to lowercase\n",
    "  removed_punctuation = remove_punctuation(lower_text) # removing punctuation\n",
    "  tokens = tokenize(removed_punctuation) # tokenize words\n",
    "  tokens = remove_stop_words(tokens) # removing stop words\n",
    "  tokens = lemmatize(tokens) # lemmatize tokens\n",
    "  tokens = remove_special_chars(tokens) # remove special characters from token\n",
    "\n",
    "  preprocessed_text = ' '.join(tokens) # final preprocessed text\n",
    "  return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "bYikUnKgre_8",
    "outputId": "c5115a9c-53da-4758-d255-bc7f0aa27ec6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def preprocess_text(text):\\n  # Remove HTML tags and special characters\\n    clean_text = re.sub('<.*?>', '', text)\\n    clean_text = re.sub(r'[^a-zA-Z\\\\s]', '', clean_text)\\n\\n    # Tokenization\\n    tokens = word_tokenize(clean_text)\\n\\n    # Remove stop words and convert to lowercase\\n    stop_words = set(stopwords.words('english'))\\n    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\\n\\n    return ' '.join(tokens)\\n    \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\\\n",
    "def preprocess_text(text):\n",
    "  # Remove HTML tags and special characters\n",
    "    clean_text = re.sub('<.*?>', '', text)\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', '', clean_text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(clean_text)\n",
    "\n",
    "    # Remove stop words and convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bzkGS2U0CpI-"
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Plot Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqXSTOuwCpI_",
    "outputId": "3c0c7f96-6ac5-4c2f-bc3e-dfeba6893ccc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Plot Summary'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KzO64nJhryAu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply function on text column\n",
    "df['ProcessedPlot'] = df['Plot Summary'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "id": "h9QUor2kJkZ4",
    "outputId": "cf2ec295-8fbb-473a-9360-278a361aa256"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Movie Title</th>\n",
       "      <th>Plot Summary</th>\n",
       "      <th>Cover Image</th>\n",
       "      <th>Year</th>\n",
       "      <th>Source</th>\n",
       "      <th>Rated</th>\n",
       "      <th>omdb Year</th>\n",
       "      <th>omdb Title</th>\n",
       "      <th>Released</th>\n",
       "      <th>...</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Language</th>\n",
       "      <th>Country</th>\n",
       "      <th>Awards</th>\n",
       "      <th>Poster</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>imdbRating</th>\n",
       "      <th>BoxOffice</th>\n",
       "      <th>imdbID</th>\n",
       "      <th>ProcessedPlot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Memory (2023 film)</td>\n",
       "      <td>Sylvia, a single mother, social worker, and re...</td>\n",
       "      <td>upload.wikimedia.org/wikipedia/en/thumb/4/4e/M...</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://en.wikipedia.org//wiki/Memory_(2023_film)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sylvia single mother social worker recovering ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Color Purple (2023 film)</td>\n",
       "      <td>In 1909 Georgia, teenager Celie Harris lives w...</td>\n",
       "      <td>upload.wikimedia.org/wikipedia/en/thumb/7/70/C...</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://en.wikipedia.org//wiki/The_Color_Purpl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>georgia teenager celie harris life sister net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Boys in the Boat (film)</td>\n",
       "      <td>The film's plot centres on the University of W...</td>\n",
       "      <td>upload.wikimedia.org/wikipedia/en/thumb/6/63/B...</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://en.wikipedia.org//wiki/The_Boys_in_the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>film plot centre university washington crew re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6093</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Ferrari (2023 film)</td>\n",
       "      <td>In the summer of 1957, Italian entrepreneur En...</td>\n",
       "      <td>upload.wikimedia.org/wikipedia/en/thumb/f/f6/F...</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://en.wikipedia.org//wiki/Ferrari_(2023_f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>summer  italian entrepreneur enzo ferrari prep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Good Grief (film)</td>\n",
       "      <td>In London, Marc enjoys a Christmas party with ...</td>\n",
       "      <td>upload.wikimedia.org/wikipedia/en/thumb/1/14/G...</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://en.wikipedia.org//wiki/Good_Grief_(film)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>london marc enjoys christmas party friend oliv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1                   Movie Title  \\\n",
       "6090           NaN            Memory (2023 film)   \n",
       "6091           NaN  The Color Purple (2023 film)   \n",
       "6092           NaN   The Boys in the Boat (film)   \n",
       "6093           NaN           Ferrari (2023 film)   \n",
       "6094           NaN             Good Grief (film)   \n",
       "\n",
       "                                           Plot Summary  \\\n",
       "6090  Sylvia, a single mother, social worker, and re...   \n",
       "6091  In 1909 Georgia, teenager Celie Harris lives w...   \n",
       "6092  The film's plot centres on the University of W...   \n",
       "6093  In the summer of 1957, Italian entrepreneur En...   \n",
       "6094  In London, Marc enjoys a Christmas party with ...   \n",
       "\n",
       "                                            Cover Image  Year  \\\n",
       "6090  upload.wikimedia.org/wikipedia/en/thumb/4/4e/M...  2023   \n",
       "6091  upload.wikimedia.org/wikipedia/en/thumb/7/70/C...  2023   \n",
       "6092  upload.wikimedia.org/wikipedia/en/thumb/6/63/B...  2023   \n",
       "6093  upload.wikimedia.org/wikipedia/en/thumb/f/f6/F...  2023   \n",
       "6094  upload.wikimedia.org/wikipedia/en/thumb/1/14/G...  2023   \n",
       "\n",
       "                                                 Source Rated omdb Year  \\\n",
       "6090  https://en.wikipedia.org//wiki/Memory_(2023_film)   NaN       NaN   \n",
       "6091  https://en.wikipedia.org//wiki/The_Color_Purpl...   NaN       NaN   \n",
       "6092  https://en.wikipedia.org//wiki/The_Boys_in_the...   NaN       NaN   \n",
       "6093  https://en.wikipedia.org//wiki/Ferrari_(2023_f...   NaN       NaN   \n",
       "6094   https://en.wikipedia.org//wiki/Good_Grief_(film)   NaN       NaN   \n",
       "\n",
       "     omdb Title Released  ... Plot Language Country Awards Poster Ratings  \\\n",
       "6090        NaN      NaN  ...  NaN      NaN     NaN    NaN    NaN     NaN   \n",
       "6091        NaN      NaN  ...  NaN      NaN     NaN    NaN    NaN     NaN   \n",
       "6092        NaN      NaN  ...  NaN      NaN     NaN    NaN    NaN     NaN   \n",
       "6093        NaN      NaN  ...  NaN      NaN     NaN    NaN    NaN     NaN   \n",
       "6094        NaN      NaN  ...  NaN      NaN     NaN    NaN    NaN     NaN   \n",
       "\n",
       "     imdbRating BoxOffice imdbID  \\\n",
       "6090        NaN       NaN    NaN   \n",
       "6091        NaN       NaN    NaN   \n",
       "6092        NaN       NaN    NaN   \n",
       "6093        NaN       NaN    NaN   \n",
       "6094        NaN       NaN    NaN   \n",
       "\n",
       "                                          ProcessedPlot  \n",
       "6090  sylvia single mother social worker recovering ...  \n",
       "6091   georgia teenager celie harris life sister net...  \n",
       "6092  film plot centre university washington crew re...  \n",
       "6093  summer  italian entrepreneur enzo ferrari prep...  \n",
       "6094  london marc enjoys christmas party friend oliv...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iuAqPENVr2PX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the preprocessed data\n",
    "preprocessed_path = '../data/processed/preprocessed_dataset.csv'\n",
    "df.to_csv(preprocessed_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textattack\n",
      "  Obtaining dependency information for textattack from https://files.pythonhosted.org/packages/69/85/f7878f69021c4f6583e07e285380d88f0bf2fafcef32c91dddd4db573692/textattack-0.3.9-py3-none-any.whl.metadata\n",
      "  Downloading textattack-0.3.9-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting bert-score>=0.3.5 (from textattack)\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting editdistance (from textattack)\n",
      "  Obtaining dependency information for editdistance from https://files.pythonhosted.org/packages/b7/a3/058d823b6285c3511dc94ed80620c3fb0c18b4aaa708f70ba71f3af28436/editdistance-0.8.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading editdistance-0.8.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.9 kB)\n",
      "Collecting flair (from textattack)\n",
      "  Obtaining dependency information for flair from https://files.pythonhosted.org/packages/af/16/536683088c7306bc51cc3cc58605759ebd83b3f7ffd05a9399f4b99c8614/flair-0.13.1-py3-none-any.whl.metadata\n",
      "  Downloading flair-0.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from textattack) (3.9.0)\n",
      "Collecting language-tool-python (from textattack)\n",
      "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
      "Collecting lemminflect (from textattack)\n",
      "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lru-dict (from textattack)\n",
      "  Obtaining dependency information for lru-dict from https://files.pythonhosted.org/packages/4e/63/a0ae20525f9d52f62ac0def47935f8a2b3b6fcd2c145218b9a27fc1fb910/lru_dict-1.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading lru_dict-1.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: datasets>=2.4.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from textattack) (2.12.0)\n",
      "Requirement already satisfied: nltk in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from textattack) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from textattack) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from textattack) (1.5.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from textattack) (1.10.1)\n",
      "Requirement already satisfied: torch!=1.8,>=1.7.0 in /Users/shreyabaral/.local/lib/python3.11/site-packages (from textattack) (2.1.2)\n",
      "Requirement already satisfied: transformers>=4.30.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from textattack) (4.36.2)\n",
      "Collecting terminaltables (from textattack)\n",
      "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tqdm in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from textattack) (4.65.0)\n",
      "Collecting word2number (from textattack)\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting num2words (from textattack)\n",
      "  Obtaining dependency information for num2words from https://files.pythonhosted.org/packages/8f/f0/ca1228af2bcbce2fdf2b23d58643c84253b88a3c1cd9dba391ca683c4b21/num2words-0.5.13-py3-none-any.whl.metadata\n",
      "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: more-itertools in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from textattack) (8.12.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from textattack) (1.7.1)\n",
      "Collecting pinyin>=0.4.0 (from textattack)\n",
      "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba (from textattack)\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting OpenHowNet (from textattack)\n",
      "  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
      "Collecting pycld2 (from textattack)\n",
      "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click<8.1.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from textattack) (8.0.4)\n",
      "Requirement already satisfied: requests in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from bert-score>=0.3.5->textattack) (2.31.0)\n",
      "Requirement already satisfied: matplotlib in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from bert-score>=0.3.5->textattack) (23.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from datasets>=2.4.0->textattack) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from datasets>=2.4.0->textattack) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from datasets>=2.4.0->textattack) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from datasets>=2.4.0->textattack) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from datasets>=2.4.0->textattack) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from datasets>=2.4.0->textattack) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from datasets>=2.4.0->textattack) (0.20.2)\n",
      "Requirement already satisfied: responses<0.19 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from datasets>=2.4.0->textattack) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from datasets>=2.4.0->textattack) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.1->textattack) (2022.7)\n",
      "Requirement already satisfied: typing-extensions in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from torch!=1.8,>=1.7.0->textattack) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from torch!=1.8,>=1.7.0->textattack) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from torch!=1.8,>=1.7.0->textattack) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from transformers>=4.30.0->textattack) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from transformers>=4.30.0->textattack) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from transformers>=4.30.0->textattack) (0.4.1)\n",
      "Collecting boto3>=1.20.27 (from flair->textattack)\n",
      "  Obtaining dependency information for boto3>=1.20.27 from https://files.pythonhosted.org/packages/a4/63/f92e8deab61d4585728d5ccbbc229def74c5b70d805fa4ddf6e5c3911ee8/boto3-1.34.39-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.34.39-py3-none-any.whl.metadata (6.6 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bpemb>=0.3.2 (from flair->textattack)\n",
      "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting conllu>=4.0 (from flair->textattack)\n",
      "  Obtaining dependency information for conllu>=4.0 from https://files.pythonhosted.org/packages/ce/3f/70a1dc5bc536755ec082b806594598a10cfffaf0de978f51d4e0e4fdfa47/conllu-4.5.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deprecated>=1.2.13 (from flair->textattack)\n",
      "  Obtaining dependency information for deprecated>=1.2.13 from https://files.pythonhosted.org/packages/20/8d/778b7d51b981a96554f29136cd59ca7880bf58094338085bcf2a979a0e6a/Deprecated-1.2.14-py2.py3-none-any.whl.metadata\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: ftfy>=6.1.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from flair->textattack) (6.1.3)\n",
      "Collecting gdown>=4.4.0 (from flair->textattack)\n",
      "  Obtaining dependency information for gdown>=4.4.0 from https://files.pythonhosted.org/packages/cb/56/f4845ed78723a4eb8eb22bcfcb46e1157a462c78c0a5ed318c68c98f9a79/gdown-5.1.0-py3-none-any.whl.metadata\n",
      "  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: gensim>=4.2.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from flair->textattack) (4.3.0)\n",
      "Collecting janome>=0.4.2 (from flair->textattack)\n",
      "  Obtaining dependency information for janome>=0.4.2 from https://files.pythonhosted.org/packages/73/7d/70f4069f4bbf0fca023e82a1fbbade6f5216365d4fe259fee1950723eca5/Janome-0.5.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langdetect>=1.0.9 (from flair->textattack)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.8.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from flair->textattack) (4.9.2)\n",
      "Collecting more-itertools (from textattack)\n",
      "  Obtaining dependency information for more-itertools from https://files.pythonhosted.org/packages/50/e2/8e10e465ee3987bb7c9ab69efb91d867d93959095f4807db102d07995d94/more_itertools-10.2.0-py3-none-any.whl.metadata\n",
      "  Downloading more_itertools-10.2.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting mpld3>=0.3 (from flair->textattack)\n",
      "  Obtaining dependency information for mpld3>=0.3 from https://files.pythonhosted.org/packages/95/6a/e3691bcc47485f38b09853207c928130571821d187cf174eed5418d45e82/mpld3-0.5.10-py3-none-any.whl.metadata\n",
      "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pptree>=3.1 (from flair->textattack)\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytorch-revgrad>=0.2.0 (from flair->textattack)\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from flair->textattack) (1.3.0)\n",
      "Collecting segtok>=1.5.11 (from flair->textattack)\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting sqlitedict>=2.0.0 (from flair->textattack)\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tabulate>=0.8.10 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from flair->textattack) (0.8.10)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair->textattack)\n",
      "  Obtaining dependency information for transformer-smaller-training-vocab>=0.2.3 from https://files.pythonhosted.org/packages/1a/cf/37ccc33c7223707c92aed9d320a03fc80474ea81876b7a25096eab6fdd59/transformer_smaller_training_vocab-0.3.3-py3-none-any.whl.metadata\n",
      "  Downloading transformer_smaller_training_vocab-0.3.3-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from flair->textattack) (1.26.16)\n",
      "Collecting wikipedia-api>=0.5.7 (from flair->textattack)\n",
      "  Obtaining dependency information for wikipedia-api>=0.5.7 from https://files.pythonhosted.org/packages/2f/3f/919727b460d88c899d110f98d1a0c415264b5d8ad8176f14ce7ad9db0e3b/Wikipedia_API-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting semver<4.0.0,>=3.0.0 (from flair->textattack)\n",
      "  Obtaining dependency information for semver<4.0.0,>=3.0.0 from https://files.pythonhosted.org/packages/9a/77/0cc7a8a3bc7e53d07e8f47f147b92b0960e902b8254859f4aee5c4d7866b/semver-3.0.2-py3-none-any.whl.metadata\n",
      "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: joblib in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from nltk->textattack) (1.2.0)\n",
      "Collecting docopt>=0.6.2 (from num2words->textattack)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting anytree (from OpenHowNet->textattack)\n",
      "  Obtaining dependency information for anytree from https://files.pythonhosted.org/packages/6a/fb/ff946843e6b55ae9fda84df3964d6c233cd2261dface789f5be02ab79bc5/anytree-2.12.1-py3-none-any.whl.metadata\n",
      "  Downloading anytree-2.12.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: setuptools in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from OpenHowNet->textattack) (68.0.0)\n",
      "Collecting botocore<1.35.0,>=1.34.39 (from boto3>=1.20.27->flair->textattack)\n",
      "  Obtaining dependency information for botocore<1.35.0,>=1.34.39 from https://files.pythonhosted.org/packages/b9/6e/0f7dd23ac395ec3db0a4dc863e2e5357f0c421009dd83b3a64dacdc5d56e/botocore-1.34.39-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.34.39-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from boto3>=1.20.27->flair->textattack) (0.10.0)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair->textattack)\n",
      "  Obtaining dependency information for s3transfer<0.11.0,>=0.10.0 from https://files.pythonhosted.org/packages/12/bb/7e7912e18cd558e7880d9b58ffc57300b2c28ffba9882b3a54ba5ce3ebc4/s3transfer-0.10.0-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting sentencepiece (from bpemb>=0.3.2->flair->textattack)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from deprecated>=1.2.13->flair->textattack) (1.14.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.4.0->textattack) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.4.0->textattack) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.4.0->textattack) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.4.0->textattack) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.4.0->textattack) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.4.0->textattack) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.4.0->textattack) (1.2.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from ftfy>=6.1.0->flair->textattack) (0.2.13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from gdown>=4.4.0->flair->textattack) (4.12.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from gensim>=4.2.0->flair->textattack) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim>=4.2.0->flair->textattack)\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: six in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from langdetect>=1.0.9->flair->textattack) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from requests->bert-score>=0.3.5->textattack) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from requests->bert-score>=0.3.5->textattack) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.2->flair->textattack) (2.2.0)\n",
      "Requirement already satisfied: protobuf in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from transformers>=4.30.0->textattack) (4.23.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim>=4.2.0->flair->textattack)\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.21.0 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from transformers>=4.30.0->textattack) (0.26.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.4)\n",
      "Requirement already satisfied: psutil in /Users/shreyabaral/anaconda3/lib/python3.11/site-packages (from accelerate>=0.21.0->transformers>=4.30.0->textattack) (5.9.0)\n",
      "Collecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim>=4.2.0->flair->textattack)\n",
      "  Obtaining dependency information for simpful from https://files.pythonhosted.org/packages/8d/93/8448d3f1aa9d2911b8cba2602aaa1af85eb31a26d28b7b737f1fa5b40c02/simpful-2.11.1-py3-none-any.whl.metadata\n",
      "  Downloading simpful-2.11.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim>=4.2.0->flair->textattack)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim>=4.2.0->flair->textattack)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading textattack-0.3.9-py3-none-any.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading editdistance-0.8.1-cp311-cp311-macosx_11_0_arm64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flair-0.13.1-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lru_dict-1.3.0-cp311-cp311-macosx_11_0_arm64.whl (11 kB)\n",
      "Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.34.39-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading transformer_smaller_training_vocab-0.3.3-py3-none-any.whl (14 kB)\n",
      "Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
      "Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.34.39-py3-none-any.whl (11.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simpful-2.11.1-py3-none-any.whl (32 kB)\n",
      "Building wheels for collected packages: pinyin, jieba, pycld2, word2number, docopt, langdetect, pptree, sqlitedict, fst-pso, miniful\n",
      "  Building wheel for pinyin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630476 sha256=0de8044e9caad711f0699737777131036aba8297c8987b290a6c1d3719d9cd81\n",
      "  Stored in directory: /Users/shreyabaral/Library/Caches/pip/wheels/1c/f5/31/ac8c91eccb570a59fe5f1471ad9f11bece8f4fd4be1ab1be25\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=db85a634a061d5f2f32a4ad6a2440015c19a5f6c730941680f4e7541e102a842\n",
      "  Stored in directory: /Users/shreyabaral/Library/Caches/pip/wheels/ac/60/cf/538a1f183409caf1fc136b5d2c2dee329001ef6da2c5084bef\n",
      "  Building wheel for pycld2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycld2: filename=pycld2-0.41-cp311-cp311-macosx_11_0_arm64.whl size=4816789 sha256=7b7ffd949af382375f5b68cc1f137eab79ec96862ae8bf9386c37f8dc95c2a20\n",
      "  Stored in directory: /Users/shreyabaral/Library/Caches/pip/wheels/9a/71/a1/89cdc6ae2ecd7bb3bf67baee27a134f761b3e2922ea7fdfbcb\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5566 sha256=fe560611f4bce4fbd551aa26cd5d7505b77920c2b6418ca843d17d8d0fbeac3d\n",
      "  Stored in directory: /Users/shreyabaral/Library/Caches/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=108c65c0410c88cfcdf81121c42e997295828d611a20f94cc17be6d87598d937\n",
      "  Stored in directory: /Users/shreyabaral/Library/Caches/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=fbdcc3902c2fd86eee848a69f04986f41470da8fcf3b03e8faf9ff6dfb2e7449\n",
      "  Stored in directory: /Users/shreyabaral/Library/Caches/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "  Building wheel for pptree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=f0181d684129646256a57e0d7c6e1063d44932f19aa5a8fc1a92e7e81092a560\n",
      "  Stored in directory: /Users/shreyabaral/Library/Caches/pip/wheels/68/8a/eb/d683aa6d09dc68ebfde2f37566ddc8807837c4415b4fd2b04c\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=a3e29bddc3fcf335282babbafc83d3a7503e768acf4af73a8ad460d5a5797cdf\n",
      "  Stored in directory: /Users/shreyabaral/Library/Caches/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=0fedf67c3562c06ffaa557d1ea4352e1636a37c97a68db0f5f08205a23aa04c4\n",
      "  Stored in directory: /Users/shreyabaral/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3513 sha256=91682bf88118812b25e76c2790056c374db385d1a9c8d70bbb305d22791de2de\n",
      "  Stored in directory: /Users/shreyabaral/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built pinyin jieba pycld2 word2number docopt langdetect pptree sqlitedict fst-pso miniful\n",
      "Installing collected packages: word2number, sqlitedict, sentencepiece, pycld2, pptree, pinyin, jieba, janome, docopt, terminaltables, semver, segtok, num2words, more-itertools, lru-dict, lemminflect, langdetect, editdistance, deprecated, conllu, anytree, wikipedia-api, simpful, OpenHowNet, miniful, language-tool-python, botocore, s3transfer, pytorch-revgrad, mpld3, gdown, fst-pso, pyfume, boto3, FuzzyTM, bert-score, transformer-smaller-training-vocab, bpemb, flair, textattack\n",
      "  Attempting uninstall: more-itertools\n",
      "    Found existing installation: more-itertools 8.12.0\n",
      "    Uninstalling more-itertools-8.12.0:\n",
      "      Successfully uninstalled more-itertools-8.12.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.76\n",
      "    Uninstalling botocore-1.29.76:\n",
      "      Successfully uninstalled botocore-1.29.76\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.4.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.4.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "aiobotocore 2.5.0 requires botocore<1.29.77,>=1.29.76, but you have botocore 1.34.39 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed FuzzyTM-2.0.5 OpenHowNet-2.0 anytree-2.12.1 bert-score-0.3.13 boto3-1.34.39 botocore-1.34.39 bpemb-0.3.4 conllu-4.5.3 deprecated-1.2.14 docopt-0.6.2 editdistance-0.8.1 flair-0.13.1 fst-pso-1.8.1 gdown-5.1.0 janome-0.5.0 jieba-0.42.1 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.3.0 miniful-0.0.6 more-itertools-10.2.0 mpld3-0.5.10 num2words-0.5.13 pinyin-0.4.0 pptree-3.1 pycld2-0.41 pyfume-0.2.25 pytorch-revgrad-0.2.0 s3transfer-0.10.0 segtok-1.5.11 semver-3.0.2 sentencepiece-0.1.99 simpful-2.11.1 sqlitedict-2.1.0 terminaltables-3.1.10 textattack-0.3.9 transformer-smaller-training-vocab-0.3.3 wikipedia-api-0.6.0 word2number-1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textattack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m augmented_text\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Apply augmentation to the 'plot' column\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maugmented_plot\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplot\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(augment_text_with_textattack)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[28], line 13\u001b[0m, in \u001b[0;36maugment_text_with_textattack\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maugment_text_with_textattack\u001b[39m(text):\n\u001b[1;32m     12\u001b[0m     augmenter \u001b[38;5;241m=\u001b[39m Augmenter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     augmented_text \u001b[38;5;241m=\u001b[39m augmenter\u001b[38;5;241m.\u001b[39maugment(text)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m augmented_text\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/textattack/augmentation/augmenter.py:125\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    122\u001b[0m words_swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodified_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m words_swapped \u001b[38;5;241m<\u001b[39m num_words_to_swap:\n\u001b[0;32m--> 125\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation(\n\u001b[1;32m    126\u001b[0m         current_text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transformation_constraints\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Get rid of transformations we already have\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    131\u001b[0m         t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transformed_texts \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_transformed_texts\n\u001b[1;32m    132\u001b[0m     ]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import Augmenter\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = {'plot': [\"This is a positive example.\",\n",
    "                 \"I don't like this movie; it's too slow.\",\n",
    "                 \"The plot is intriguing and captivating.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to apply TextAttack augmentation\n",
    "def augment_text_with_textattack(text):\n",
    "    augmenter = Augmenter('wordnet')\n",
    "    augmented_text = augmenter.augment(text)\n",
    "    return augmented_text\n",
    "\n",
    "# Apply augmentation to the 'plot' column\n",
    "df['augmented_plot'] = df['plot'].apply(augment_text_with_textattack)\n",
    "\n",
    "# Display the results\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
